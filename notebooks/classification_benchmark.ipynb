{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4477950e",
   "metadata": {},
   "source": [
    "# OIKAN Classification Benchmark Tests\n",
    "\n",
    "This notebook evaluates the OIKANClassifier on various classification tasks to assess:\n",
    "1. Classification metrics (Accuracy, F1-score, ROC-AUC)\n",
    "2. Decision boundary interpretability\n",
    "3. Symbolic formula extraction quality\n",
    "4. Comparison with traditional classifiers\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2c56f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "!pip install -qU oikan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f3f9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze | grep oikan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736e74c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    f1_score\n",
    ")\n",
    "from sklearn.datasets import (\n",
    "    make_classification,\n",
    "    make_moons,\n",
    "    make_circles,\n",
    "    load_iris,\n",
    "    load_breast_cancer\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from oikan.model import OIKANClassifier\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99bc002",
   "metadata": {},
   "source": [
    "## 1. Synthetic Classification Tests\n",
    "\n",
    "First, let's evaluate OIKAN on synthetic datasets with known decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0823e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_datasets():\n",
    "    datasets = {\n",
    "        'Linear': make_classification(\n",
    "            n_samples=1000, n_features=2, n_redundant=0,\n",
    "            n_informative=2, random_state=42,\n",
    "            n_clusters_per_class=1\n",
    "        ),\n",
    "        'Moons': make_moons(\n",
    "            n_samples=1000, noise=0.1, random_state=42\n",
    "        ),\n",
    "        'Circles': make_circles(\n",
    "            n_samples=1000, noise=0.1, factor=0.3, random_state=42\n",
    "        )\n",
    "    }\n",
    "    return datasets\n",
    "\n",
    "synthetic_datasets = generate_synthetic_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3356e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_classifier(model, X, y, model_name=\"OIKAN\"):\n",
    "    results = {}\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Training time\n",
    "    start_time = time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time() - start_time\n",
    "    \n",
    "    # Prediction time\n",
    "    start_time = time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    predict_time = time() - start_time\n",
    "    \n",
    "    # Metrics\n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'F1': f1_score(y_test, y_pred, average='weighted'),\n",
    "        'Train Time': train_time,\n",
    "        'Predict Time': predict_time,\n",
    "        'Report': classification_report(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    if model_name == 'OIKAN':\n",
    "        symbolic_pred = model.symbolic_predict(X_test)\n",
    "        results['Symbolic Accuracy'] = accuracy_score(y_test, symbolic_pred)\n",
    "        results['Symbolic Formula'] = model.get_symbolic_formula()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba785e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_synthetic_benchmarks():\n",
    "    results = []\n",
    "    models = {\n",
    "        'OIKAN': OIKANClassifier(hidden_dims=[32, 16]),\n",
    "        'MLPClassifier': MLPClassifier(hidden_layer_sizes=(32, 16), max_iter=500),\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100),\n",
    "        'LogisticRegression': LogisticRegression(),\n",
    "        'DecisionTree': DecisionTreeClassifier(),\n",
    "        'SVM': SVC(probability=True)\n",
    "    }\n",
    "    \n",
    "    metrics = ['accuracy', 'roc_auc', 'train_time', 'predict_time']\n",
    "    \n",
    "    for dataset_name, (X, y) in synthetic_datasets.items():\n",
    "        print(f\"\\nBenchmarking {dataset_name} dataset...\")\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            res = benchmark_classifier(model, X, y, model_name)\n",
    "            res['Dataset'] = dataset_name\n",
    "            results.append(res)\n",
    "            print(f\"{model_name}:\")\n",
    "            print(f\"Accuracy = {res['Accuracy']:.4f}\")\n",
    "            if 'ROC AUC' in res:\n",
    "                print(f\"ROC AUC = {res['ROC AUC']:.4f}\")\n",
    "            \n",
    "            if model_name == 'OIKAN':\n",
    "                print(f\"Symbolic Accuracy = {res['Symbolic Accuracy']:.4f}\")\n",
    "                print(\"Example decision boundary terms:\")\n",
    "                for i, formula in enumerate(res['Symbolic Formula'][0]):\n",
    "                    print(f\"Class {i}: {formula[:100]}...\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "benchmark_results = run_synthetic_benchmarks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3d6e15",
   "metadata": {},
   "source": [
    "## 2. Real Dataset Tests\n",
    "\n",
    "Now let's evaluate OIKAN on real-world classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c8d777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_real_datasets():\n",
    "    datasets = {\n",
    "        'Iris': load_iris(return_X_y=True),\n",
    "        'Breast Cancer': load_breast_cancer(return_X_y=True)\n",
    "    }\n",
    "    return datasets\n",
    "\n",
    "real_datasets = load_real_datasets()\n",
    "real_results = []\n",
    "\n",
    "for dataset_name, (X, y) in real_datasets.items():\n",
    "    print(f\"\\nBenchmarking {dataset_name} dataset...\")\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Run benchmarks\n",
    "    oikan = OIKANClassifier(hidden_dims=[64, 32])\n",
    "    res = benchmark_classifier(oikan, X_scaled, y, 'OIKAN')\n",
    "    res['Dataset'] = dataset_name\n",
    "    real_results.append(res)\n",
    "    \n",
    "    print(f\"OIKAN Accuracy = {res['Accuracy']:.4f}\")\n",
    "    print(f\"Symbolic Accuracy = {res['Symbolic Accuracy']:.4f}\")\n",
    "    print(\"\\nExample decision boundary formula:\")\n",
    "    print(res['Symbolic Formula'][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7125d1de",
   "metadata": {},
   "source": [
    "## 3. Results Analysis\n",
    "\n",
    "Let's analyze the classification performance metrics in a clear tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d97e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_benchmark_results(synthetic_results, real_results):\n",
    "    # Combine all results\n",
    "    all_results = pd.concat([pd.DataFrame(synthetic_results), pd.DataFrame(real_results)])\n",
    "    \n",
    "    # Format results table with consistent columns\n",
    "    summary = all_results.pivot_table(\n",
    "        index=['Dataset', 'Model'],\n",
    "        values=['Accuracy', 'F1', 'Train Time', 'Predict Time'],\n",
    "        aggfunc='mean'\n",
    "    ).round(4)\n",
    "    \n",
    "    # Sort by dataset and accuracy\n",
    "    summary = summary.sort_index(level=0)\n",
    "    \n",
    "    print(\"\\nClassification Benchmark Results:\")\n",
    "    print(\"==============================\\n\")\n",
    "    print(summary)\n",
    "    \n",
    "    # For OIKAN models, show symbolic accuracy\n",
    "    oikan_results = all_results[all_results['Model'] == 'OIKAN']\n",
    "    print(\"\\nOIKAN Symbolic Formula Performance:\")\n",
    "    print(\"================================\\n\")\n",
    "    for _, row in oikan_results.iterrows():\n",
    "        print(f\"Dataset: {row['Dataset']}\")\n",
    "        print(f\"Neural Accuracy = {row['Accuracy']:.4f}\")\n",
    "        print(f\"Symbolic Accuracy = {row['Symbolic Accuracy']:.4f}\\n\")\n",
    "\n",
    "display_benchmark_results(benchmark_results, pd.DataFrame(real_results))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
